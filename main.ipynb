{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df_path):\n",
    "\n",
    "    df = pd.read_csv(df_path)\n",
    "    df['Extracurricular Activities'] = df['Extracurricular Activities'].map({'Yes': 1, 'No': 0})\n",
    "    df.to_csv(\"dataset/Student_Performance_preprocess.csv\", index = False)\n",
    "\n",
    "preprocess_df(df_path=\"dataset/Student_Performance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/Student_Performance_preprocess.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Studied  Previous Scores  Extracurricular Activities  Sleep Hours  \\\n",
       "0              7               99                           1            9   \n",
       "1              4               82                           0            4   \n",
       "2              8               51                           1            7   \n",
       "3              5               52                           1            5   \n",
       "4              7               75                           0            8   \n",
       "\n",
       "   Sample Question Papers Practiced  Performance Index  \n",
       "0                                 1               91.0  \n",
       "1                                 2               65.0  \n",
       "2                                 2               45.0  \n",
       "3                                 2               36.0  \n",
       "4                                 5               66.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vas = df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']\n",
      "Performance Index\n"
     ]
    }
   ],
   "source": [
    "print(vas[:-1])\n",
    "print(vas[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent Values;\n",
      "['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']\n",
      "dependent Values;\n",
      "Performance Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(csv_file):\n",
    "    # Step 1: Load the CSV file using pandas\n",
    "    data = pd.read_csv(csv_file)\n",
    "    data_col_list = data.columns.to_list()\n",
    "    x_col_list = data_col_list[:-1]\n",
    "    print(f\"Independent Values;\\n{x_col_list}\")\n",
    "\n",
    "    y_col = data_col_list[-1]\n",
    "    # Step 2: Extract the independent variables (features) into matrix X\n",
    "    # Assuming that the independent variables are in columns 'X1', 'X2', ..., 'Xn'\n",
    "    # X = data[['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']].values\n",
    "    X = data[x_col_list].values\n",
    "\n",
    "    # Step 3: Extract the dependent variable into vector Y\n",
    "    # Assuming that the dependent variable is in a column named 'Y'\n",
    "    Y = data[y_col].values\n",
    "    print(f\"dependent Values;\\n{y_col}\")\n",
    "    return X, Y\n",
    "\n",
    "# Usage:\n",
    "csv_file = 'dataset/Student_Performance_preprocess.csv'  # Replace with the path to your CSV file\n",
    "X, Y = load_data(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 99  1  9  1]\n",
      " [ 4 82  0  4  2]\n",
      " [ 8 51  1  7  2]\n",
      " ...\n",
      " [ 6 83  1  8  5]\n",
      " [ 9 97  1  7  0]\n",
      " [ 7 74  0  8  1]]\n",
      "<class 'numpy.ndarray'>\n",
      "----\n",
      "[91. 65. 45. ... 74. 95. 64.]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(type(X))\n",
    "print(\"----\")\n",
    "print(Y)\n",
    "print(type(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(X, Y, test_size=0.3, random_state=None):\n",
    "    \"\"\"\n",
    "    Split the data into training and test sets.\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): The independent variables (features).\n",
    "    Y (numpy.ndarray): The dependent variable.\n",
    "    test_size (float): The proportion of data to include in the test split (default is 0.3).\n",
    "    random_state (int or None): Seed for random number generation (optional).\n",
    "\n",
    "    Returns:\n",
    "    X_train (numpy.ndarray): Training data for independent variables.\n",
    "    X_test (numpy.ndarray): Test data for independent variables.\n",
    "    Y_train (numpy.ndarray): Training data for the dependent variable.\n",
    "    Y_test (numpy.ndarray): Test data for the dependent variable.\n",
    "    \"\"\"\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "# Usage:\n",
    "X_train, X_test, Y_train, Y_test = split_data(X, Y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5 69  0  8  2]\n",
      " [ 2 46  1  4  8]\n",
      " [ 7 56  1  7  5]\n",
      " ...\n",
      " [ 5 48  1  6  4]\n",
      " [ 3 86  1  9  5]\n",
      " [ 6 43  1  6  4]]\n",
      "lenght of array 3000\n",
      "<class 'numpy.ndarray'>\n",
      "----\n",
      "[[ 4 99  1  6  1]\n",
      " [ 6 90  1  9  3]\n",
      " [ 8 57  0  6  1]\n",
      " ...\n",
      " [ 9 48  0  7  6]\n",
      " [ 1 47  0  9  0]\n",
      " [ 2 46  0  6  6]]\n",
      "lenght of array 7000\n",
      "<class 'numpy.ndarray'>\n",
      "----\n",
      "[51. 20. 46. ... 33. 69. 29.]\n",
      "lenght of array 3000\n",
      "<class 'numpy.ndarray'>\n",
      "----\n",
      "[82. 79. 50. ... 44. 20. 24.]\n",
      "lenght of array 7000\n",
      "<class 'numpy.ndarray'>\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "print(X_test)\n",
    "print(f'lenght of array {len(X_test)}')\n",
    "print(type(X_test))\n",
    "print(\"----\")\n",
    "\n",
    "print(X_train)\n",
    "print(f'lenght of array {len(X_train)}')\n",
    "print(type(X_train))\n",
    "print(\"----\")\n",
    "\n",
    "print(Y_test)\n",
    "print(f'lenght of array {len(Y_test)}')\n",
    "print(type(Y_test))\n",
    "print(\"----\")\n",
    "\n",
    "print(Y_train)\n",
    "print(f'lenght of array {len(Y_train)}')\n",
    "print(type(Y_train))\n",
    "print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost_function(X, Y, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute the mean squared error (MSE) cost for a given hypothesis (model).\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): Training data for independent variables.\n",
    "    Y (numpy.ndarray): Actual values (dependent variable).\n",
    "    weights (numpy.ndarray): Model coefficients (weights).\n",
    "    bias (float): Model bias term.\n",
    "\n",
    "    Returns:\n",
    "    cost (float): Mean squared error cost.\n",
    "    \"\"\"\n",
    "    # Compute the predicted values using the hypothesis function\n",
    "    predicted_values = H_function(X, weights, bias)\n",
    "    \n",
    "    # Calculate the squared differences between predicted and actual values\n",
    "    squared_errors = (predicted_values - Y) ** 2\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    cost = np.mean(squared_errors)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "# Usage:\n",
    "# Assuming you have already trained your linear regression model and obtained the weights and bias.\n",
    "# cost_train = Cost_function(X_train, Y_train, weights, bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_function(X, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute the estimated prices for all examples in the training set.\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): Training data for independent variables.\n",
    "    weights (numpy.ndarray): Model coefficients (weights).\n",
    "    bias (float): Model bias term.\n",
    "\n",
    "    Returns:\n",
    "    estimated_prices (numpy.ndarray): Estimated prices for all examples in the training set.\n",
    "    \"\"\"\n",
    "    estimated_prices = np.dot(X, weights) + bias\n",
    "    return estimated_prices\n",
    "\n",
    "# Usage:\n",
    "# Assuming you have already trained your linear regression model and obtained the weights and bias.\n",
    "# estimated_prices_train = H_function(X_train, updated_weights, updated_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train_Cost = 3399.6320960471517  //  Test_Cost = 180.83346000725092\n",
      "Epoch 100: Train_Cost = 3392.8525883512116  //  Test_Cost = 207.01675792897746\n",
      "Epoch 200: Train_Cost = 3386.0866174866055  //  Test_Cost = 236.447737625976\n",
      "Epoch 300: Train_Cost = 3379.334156422963  //  Test_Cost = 269.11663812487006\n",
      "Epoch 400: Train_Cost = 3372.595178183886  //  Test_Cost = 305.01372118539314\n",
      "Epoch 500: Train_Cost = 3365.869655846847  //  Test_Cost = 344.1292712517971\n",
      "Epoch 600: Train_Cost = 3359.1575625430783  //  Test_Cost = 386.45359540437335\n",
      "Epoch 700: Train_Cost = 3352.458871457466  //  Test_Cost = 431.97702331107143\n",
      "Epoch 800: Train_Cost = 3345.7735558284435  //  Test_Cost = 480.6899071792123\n",
      "Epoch 900: Train_Cost = 3339.1015889478836  //  Test_Cost = 532.5826217072976\n",
      "Epoch 1000: Train_Cost = 3332.44294416099  //  Test_Cost = 587.6455640369398\n",
      "Epoch 1100: Train_Cost = 3325.797594866194  //  Test_Cost = 645.8691537048553\n",
      "Epoch 1200: Train_Cost = 3319.165514515047  //  Test_Cost = 707.2438325949963\n",
      "Epoch 1300: Train_Cost = 3312.5466766121153  //  Test_Cost = 771.7600648907556\n",
      "Epoch 1400: Train_Cost = 3305.9410547148696  //  Test_Cost = 839.4083370272665\n",
      "Epoch 1500: Train_Cost = 3299.3486224335875  //  Test_Cost = 910.1791576438326\n",
      "Epoch 1600: Train_Cost = 3292.76935343124  //  Test_Cost = 984.0630575364086\n",
      "Epoch 1700: Train_Cost = 3286.203221423392  //  Test_Cost = 1061.050589610225\n",
      "Epoch 1800: Train_Cost = 3279.6502001780937  //  Test_Cost = 1141.132328832484\n",
      "Epoch 1900: Train_Cost = 3273.1102635157786  //  Test_Cost = 1224.2988721851507\n",
      "Epoch 2000: Train_Cost = 3266.583385309155  //  Test_Cost = 1310.5408386178613\n",
      "Epoch 2100: Train_Cost = 3260.0695394831064  //  Test_Cost = 1399.848869000926\n",
      "Epoch 2200: Train_Cost = 3253.5687000145845  //  Test_Cost = 1492.2136260783807\n",
      "Epoch 2300: Train_Cost = 3247.080840932505  //  Test_Cost = 1587.625794421228\n",
      "Epoch 2400: Train_Cost = 3240.605936317645  //  Test_Cost = 1686.0760803806643\n",
      "Epoch 2500: Train_Cost = 3234.1439603025387  //  Test_Cost = 1787.5552120415034\n",
      "Epoch 2600: Train_Cost = 3227.6948870713736  //  Test_Cost = 1892.0539391756572\n",
      "Epoch 2700: Train_Cost = 3221.2586908598905  //  Test_Cost = 1999.5630331956595\n",
      "Epoch 2800: Train_Cost = 3214.835345955276  //  Test_Cost = 2110.073287108393\n",
      "Epoch 2900: Train_Cost = 3208.4248266960617  //  Test_Cost = 2223.5755154687904\n",
      "Epoch 3000: Train_Cost = 3202.0271074720226  //  Test_Cost = 2340.06055433377\n",
      "Epoch 3100: Train_Cost = 3195.6421627240743  //  Test_Cost = 2459.5192612161186\n",
      "Epoch 3200: Train_Cost = 3189.269966944171  //  Test_Cost = 2581.9425150385873\n",
      "Epoch 3300: Train_Cost = 3182.9104946752004  //  Test_Cost = 2707.321216088014\n",
      "Epoch 3400: Train_Cost = 3176.5637205108883  //  Test_Cost = 2835.6462859695503\n",
      "Epoch 3500: Train_Cost = 3170.2296190956913  //  Test_Cost = 2966.9086675610442\n",
      "Epoch 3600: Train_Cost = 3163.9081651246975  //  Test_Cost = 3101.09932496741\n",
      "Epoch 3700: Train_Cost = 3157.599333343527  //  Test_Cost = 3238.2092434751926\n",
      "Epoch 3800: Train_Cost = 3151.303098548231  //  Test_Cost = 3378.2294295071474\n",
      "Epoch 3900: Train_Cost = 3145.0194355851854  //  Test_Cost = 3521.1509105769615\n",
      "Epoch 4000: Train_Cost = 3138.7483193509975  //  Test_Cost = 3666.9647352440825\n",
      "Epoch 4100: Train_Cost = 3132.4897247924023  //  Test_Cost = 3815.6619730685775\n",
      "Epoch 4200: Train_Cost = 3126.2436269061627  //  Test_Cost = 3967.2337145661304\n",
      "Epoch 4300: Train_Cost = 3120.010000738971  //  Test_Cost = 4121.671071163143\n",
      "Epoch 4400: Train_Cost = 3113.788821387346  //  Test_Cost = 4278.96517515189\n",
      "Epoch 4500: Train_Cost = 3107.580063997537  //  Test_Cost = 4439.107179645786\n",
      "Epoch 4600: Train_Cost = 3101.383703765421  //  Test_Cost = 4602.088258534752\n",
      "Epoch 4700: Train_Cost = 3095.1997159364087  //  Test_Cost = 4767.899606440647\n",
      "Epoch 4800: Train_Cost = 3089.0280758053395  //  Test_Cost = 4936.532438672884\n",
      "Epoch 4900: Train_Cost = 3082.868758716387  //  Test_Cost = 5107.9779911839305\n",
      "Epoch 5000: Train_Cost = 3076.7217400629593  //  Test_Cost = 5282.227520525164\n",
      "Epoch 5100: Train_Cost = 3070.586995287601  //  Test_Cost = 5459.272303802614\n",
      "Epoch 5200: Train_Cost = 3064.464499881894  //  Test_Cost = 5639.103638632896\n",
      "Epoch 5300: Train_Cost = 3058.3542293863593  //  Test_Cost = 5821.712843099224\n",
      "Epoch 5400: Train_Cost = 3052.2561593903633  //  Test_Cost = 6007.091255707473\n",
      "Epoch 5500: Train_Cost = 3046.1702655320146  //  Test_Cost = 6195.230235342366\n",
      "Epoch 5600: Train_Cost = 3040.096523498069  //  Test_Cost = 6386.121161223799\n",
      "Epoch 5700: Train_Cost = 3034.034909023834  //  Test_Cost = 6579.755432863119\n",
      "Epoch 5800: Train_Cost = 3027.98539789307  //  Test_Cost = 6776.124470019611\n",
      "Epoch 5900: Train_Cost = 3021.9479659378944  //  Test_Cost = 6975.219712657078\n"
     ]
    }
   ],
   "source": [
    "def Gradient_descent(X_train, Y_train, X_test, Y_test, weights, bias, learning_rate, num_epochs):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to update the model parameters (weights and bias).\n",
    "\n",
    "    Args:\n",
    "    X (numpy.ndarray): Training data for independent variables.\n",
    "    Y (numpy.ndarray): Actual values (dependent variable).\n",
    "    weights (numpy.ndarray): Model coefficients (weights).\n",
    "    bias (float): Model bias term.\n",
    "    learning_rate (float): Learning rate for gradient descent.\n",
    "    num_epochs (int): Number of epochs for training.\n",
    "\n",
    "    Returns:\n",
    "    weights (numpy.ndarray): Updated model coefficients (weights).\n",
    "    bias (float): Updated model bias term.\n",
    "    cost_history (list): List of cost values for each epoch.\n",
    "    \"\"\"\n",
    "    train_cost_history = []\n",
    "    test_cost_history = []\n",
    "\n",
    "\n",
    "    # Optional: Normalize the data\n",
    "    X_mean = np.mean(X_train, axis=0)\n",
    "    X_std = np.std(X_train, axis=0)\n",
    "    X_normalized = (X_train - X_mean) / X_std\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Compute the predicted values using the current model\n",
    "        predicted_values = H_function(X_normalized, weights, bias)\n",
    "        \n",
    "        # Compute the gradients of the weights and bias\n",
    "        gradient_weights = (1 / len(Y_train)) * np.dot(X_normalized.T, (predicted_values - Y_train))\n",
    "        gradient_bias = (1 / len(Y_train)) * np.sum(predicted_values - Y_train)\n",
    "        \n",
    "        # Update the weights and bias using the gradients and learning rate\n",
    "        weights -= learning_rate * gradient_weights\n",
    "        bias -= learning_rate * gradient_bias\n",
    "        \n",
    "        # Calculate the cost and append it to the cost history\n",
    "        train_cost = Cost_function(X_normalized, Y_train, weights, bias)\n",
    "        # cost_history.append(cost)\n",
    "        train_cost_history.append(train_cost)\n",
    "        \n",
    "        test_cost = Cost_function(X_test, Y_test, weights, bias)\n",
    "        test_cost_history.append(test_cost)\n",
    "        \n",
    "        # Print the cost for every 100 epochs (optional)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Train_Cost = {train_cost}  //  Test_Cost = {test_cost}\")\n",
    "\n",
    "    return weights, bias, train_cost_history, test_cost_history\n",
    "\n",
    "# Usage:\n",
    "# Define your learning rate and number of epochs\n",
    "learning_rate = 0.00001  # Adjust as needed\n",
    "num_epochs = 6000\n",
    "\n",
    "# Initialize your weights and bias (you can use random values or zeros)\n",
    "initial_weights = np.random.rand(X_train.shape[1])\n",
    "initial_bias = 0.05\n",
    "\n",
    "# Perform gradient descent\n",
    "updated_weights, updated_bias, train_cost_history, test_cost_history = Gradient_descent(X_train, Y_train, X_test, Y_test, initial_weights, initial_bias, learning_rate, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmoAAAGDCAYAAACbcTyoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA/q0lEQVR4nO3de5zVVb3/8ddnRi4iCDgoIliA4A2EQSc0LzgcJelIYT601FA8WaTHS1YqEiUDR05GvzykZmXeRUUPeStvqTkhJ0XBSEFUEEkR8kKCjKjA8Pn98f0ObIY9M/vy/e7r+/l47Mfsvfb+fr9rr0F4u9Z3rWXujoiIiIgUnop8V0BEREREklNQExERESlQCmoiIiIiBUpBTURERKRAKaiJiIiIFCgFNREREZECpaAmIpIGM1tpZsfn6do9zWyumW0ws1/kow7NmVm9mX073/UQKVUKaiJlyMzOMLMFZtZgZmvM7FEzOzrLc+Y8wJhZXzNzM3u4WfksM6vLZV1yZALwAbC7u/+w+ZtmdquZbQp/r02Pv+e+miISFQU1kTJjZj8AZgL/DfQEPgdcD4zNY7WydYSZHZXvSqTDzHbJ4LDPA6946yuVz3D3zgmPoRlWUUQKgIKaSBkxs67ANOB8d7/P3T92983u/gd3vzT8TAczm2lmq8PHTDPrEL7Xw8z+aGbrzOxfZvaMmVWY2R0Ege8PYS/OZUmuvdTMxiS83sXMPjCzQ82sY9gLtjY89wtm1jONrzYDuLKF73y2mc1rVuZmNiB8fquZXR/2KjaY2f+Z2d7h9/7QzF41s2HNTvsFM3slfP8WM+uYcO4xZrYo/B5/NbMhCe+tNLOJZvYS8HGysGZmR4bff33488imegLjgcvCeqbVe5nQ+zgh/L2uMbMfJrzf4u89fH9s+L0+MrM3zGx0wuk/H7bbBjP7k5n1CI/J9vcqUvYU1ETKyxeBjsD9rXxmMnAEUA0MBYYDPw7f+yGwCtiToDfuR4C7+5nAW8BXwl6cGUnOezdwesLrE4AP3P1FggDSFdgXqALOBT5J43v9Ctg/i6HXrxN8xx7AZ8CzwIvh6znA1c0+/82w/vsB+4fHYmaHAjcD3w2/x2+BhxIDD0EbnAh0c/ctiSc1sz2Ah4FrwuOvBh42syp3Pxu4k+09Zk9m+F1HAgOBLwGXJ7RZi793MxsO3A5cCnQDRgArE855BvAfwF5Ae+CSsDzb36tI2VNQEykvVQThaEsrn/kmMM3d33P394GpwJnhe5uBXsDnw564Z9oYhkt0F/BVM+sUvj4jLGs6bxUwwN0b3X2hu3+Uxvf6FJhOC71qKbg/vOanBCH2U3e/3d0bgXuA5j1q17n72+7+r/C6TQH0O8Bv3X1++D1uIwh+RyQce014bLLAciKwzN3vcPct7n438CrwlTS+yyVh71XT47Zm708Ne1JfBm5JqHtrv/dzgJvd/Ql33+ru77j7qwnnvMXdXw+/070EYQ+y/72KlD0FNZHyshbo0cb9UfsA/0h4/Y+wDODnwHLgT2a2wswuT/XC7r4cWAp8JQxrX2V7ULsDeByYHQ67zTCzdqmeO/Q7oKeZpRNqmryb8PyTJK87N/v82wnPE9vn88APE4MSQW/SPi0c21zztm86f+9Wa7+j/+fu3RIe41Ose2u/932BN1q55j8Tnm9ke3tF8XsVKWsKaiLl5VmC3qeTWvnMaoLA0eRzYRnuvsHdf+ju/Ql6eX5gZseFn0ulZ61p+HMswU3xy8Pzbnb3qe5+MHAkMAY4K+VvFZ6DoBfovwBLeOtjoKkXDzPbO53ztmDfhOfb2ocgBE1vFpQ6hT1j26raynmbt33T+d/JusbbtVT3Fn/vBN9rv3QvFMXvVaTcKaiJlBF3Xw9cAfzKzE4ys05m1s7MvmxmTfeV3Q382Mz2DG8KvwKYBdtulB9gZgZ8BDSGDwh6ofq3UYXZBPdGncf23jTMbKSZHWJmleF5NyecNx13AB2AxBvd/w4MMrPq8Kb/ugzO29z5ZtYnvKfsRwTDoxD06p1rZodbYDczO9HMuqR43kcI7rU7I5xs8Q3gYOCPEdS5yU/C3/sggvvKmure4u8duAn4DzM7zoLJI73N7MC2LhTh71WkbCmoiZQZd78a+AHBjeLvE/SWXAA8EH7kSmAB8BLwMsFN9U33fg0EngQaCHrnrnf3+vC9nxL8Q7/OzJpuJm9+7TXhcUeyPSAA7E1w0/5HBMOjf2F7OPyNmf0mxe/WCEwB9kgoe51gpuuTwDJgXvKj03IX8CdgRfi4MrzWAoL71K4DPiQYJj471ZO6+1qCXqcfEgxTXwaMcfcP0qhb06zQpkfzY/8S1uspgmHSP4XlLf7e3f15glD3P8D68BzNe/6SafH3KiKpsdTvAxYRkWJlZn2BN4F2bUwmEZECoh41ERERkQKloCYiIiJSoDT0KSIiIlKg1KMmIiIiUqAU1EREREQKVGurkxe1Hj16eN++fWO9xscff8xuu+0W6zXKjdo0WmrP6KlNo6X2jJ7aNFq5as+FCxd+4O57Ni8v2aDWt29fFixYEOs16uvrqa2tjfUa5UZtGi21Z/TUptFSe0ZPbRqtXLWnmTXfPg7Q0KeIiIhIwVJQExERESlQCmoiIiIiBapk71FLZvPmzaxatYpPP/00kvN17dqVpUuXRnKuYtSxY0f69OlDu3bt8l0VERGRklRWQW3VqlV06dKFvn37YmZZn2/Dhg106dIlgpoVH3dn7dq1rFq1in79+uW7OiIiIiWprIY+P/30U6qqqiIJaeXOzKiqqoqsd1JERER2VlZBDVBIi5DaUkREJF5lF9Tyae3atVRXV1NdXc3ee+9N7969t73etGlTq8cuWLCAiy66KK3rNTQ08N3vfpf99tuPQYMGMWLECObPn592vWfOnMnGjRvTPk5ERESyU1b3qKXrgb+9w88ff43V6z5hn267cukJB3DSsN4Zn6+qqopFixYBUFdXR+fOnbnkkku2vb9lyxZ22SX5r6Smpoaampq0rvftb3+bfv36sWzZMioqKlixYkVGkx9mzpzJuHHj6NSpU9rHioiISObUo9aCB/72DpPue5l31n2CA++s+4RJ973MA397J9LrnH322fzgBz9g5MiRTJw4keeff54jjzySYcOGceSRR/Laa68BwcrIY8aMAYKQ961vfYva2lr69+/PNddcs9N533jjDebPn8+VV15JRUXwa+7fvz8nnngiAFdffTWDBw9m8ODBzJw5Ewi2yTjxxBMZOnQogwcP5p577uGaa65h9erVjBw5kpEjR0b63UVERArSvJnw5twdy96cG5TnWNn2qE39wxJeWf1Ri+//7a11bGrcukPZJ5sbuWzOS9z9/FsANDY2UllZue39g/fZnSlfGZR2XV5//XWefPJJKisr+eijj5g7dy677LILTz75JD/60Y/4/e9/v9Mxr776Kk8//TQbNmzggAMO4LzzztthmYwlS5ZQXV29Q/2aLFy4kFtuuYX58+fj7hx++OEce+yxrFixgn322YeHH34YgPXr19O1a1euvvpqnn76aXr06JH2dxMRESkq19bA2mWAwfiHgrK7ToPXHw2eH31xTqtTtkGtLc1DWlvl2Tj11FO3Bar169czfvx4li1bhpmxefPmpMeceOKJdOjQgQ4dOrDXXnvx7rvv0qdPn5SuN2/ePL72ta9t22T25JNP5plnnmH06NFccsklTJw4kTFjxnDMMcdE8wVFRESKwaxTwpAG4HDbVzmycjdobAiKOnTNeZXKNqi11fN11FV/5p11n+xU3rvbrtzz3S8C0a2j1hSYAH7yk58wcuRI7r//flauXNniRrAdOnTY9ryyspItW7bs8P6gQYP4+9//ztatW7cNfTZx96Tn3H///Vm4cCGPPPIIkyZN4ktf+hJXXHFFht9KRESkiMw6BZY/0azQad8U0gAmvZXTKoHuUWvRpSccwK7tdhw23LVdJZeecECs112/fj29ewcTFm699daMz7PffvtRU1PDlClTtgWzZcuW8eCDDzJixAgeeOABNm7cyMcff8z999/PMcccw+rVq+nUqRPjxo3jkksu4cUXXwSgS5cubNiwIevvJiIiUpDmzUwS0poZ/4ecVKU5BbUWnDSsNz89+RB6d9sVI+hJ++nJh2Q16zMVl112GZMmTeKoo46isbExq3PdeOON/POf/2TAgAEccsghfOc732Gfffbh0EMP5eyzz2b48OEcfvjhfPvb32bYsGG8/PLLDB8+nOrqaqZPn86Pf/xjACZMmMCXv/xlTSYQEZHSM28mPDmlxbe3jUHd9tWdJxjkgLU0DFbsampqfMGCBTuULV26lIMOOiiya5TzFlJNom7T+vr6Fod7JX1qz+ipTaOl9oye2jQNbYQ0CILatuXdKzvCT96NpSpmttDdd1qHK7YeNTM7wMwWJTw+MrOLzWwPM3vCzJaFP7snHDPJzJab2WtmdkJC+WFm9nL43jWmJfFFREQkG7NOaTOk7aQx99smxhbU3P01d69292rgMGAjcD9wOfCUuw8EngpfY2YHA6cBg4DRwPVm1nST2K+BCcDA8DE6rnqLiIhIiUvlnrRk6tZHXpW25OoeteOAN9z9H8BY4Law/DbgpPD5WGC2u3/m7m8Cy4HhZtYL2N3dn/VgnPb2hGNEREREUpfCcGeibTeI5SGkQe6C2mnA3eHznu6+BiD8uVdY3ht4O+GYVWFZ7/B583IRERGR1KUZ0rbJU0iDHKyjZmbtga8Ck9r6aJIyb6U82bUmEAyR0rNnT+rr63d4v2vXrpEuM9HY2Fj2y1Z8+umnO7VzNhoaGiI9X7lTe0ZPbRottWf01KbJ7fvWffRbcdtOoaLpdfNgsTV875GaO+mcx/bMxYK3XwZedPemaRLvmlkvd18TDmu+F5avAvZNOK4PsDos75OkfCfufgNwAwSzPpvPelm6dGmkszQ16xM6duzIsGHDIjufZitFS+0ZPbVptNSe0VObJjHrFFjR+j1pzQNcJUDdejrnuT1zMfR5OtuHPQEeAsaHz8cDDyaUn2ZmHcysH8GkgefD4dENZnZEONvzrIRjisratWuprq6murqavffem969e297vWnTpjaPr6+v569//WuL7z/66KPU1NRw0EEHceCBB3LJJZekXcdFixbxyCOPpH2ciIhIQSqiiQPJxBrUzKwTMAq4L6H4KmCUmS0L37sKwN2XAPcCrwCPAee7e9OKr+cBNxJMMHgDeDTOegPBL7b5wnZvzg3KM1RVVcWiRYtYtGgR5557Lt///ve3vW7fvn2bx7cW1BYvXswFF1zArFmzWLp0KYsXL6Z///5p11FBTURESkYR3pPWXKxBzd03unuVu69PKFvr7se5+8Dw578S3pvu7vu5+wHu/mhC+QJ3Hxy+d4HnYpXe3ofC/569Pay9OTd43fvQSC+zcOFCjj32WA477DBOOOEE1qxZA8A111zDwQcfzJAhQzjttNNYuXIlv/nNb/if//kfqqureeaZZ3Y4z4wZM5g8eTIHHnggALvssgv/+Z//CcA//vEPjjvuOIYMGcJxxx3HW28Fe5X97//+L4MHD2bo0KGMGDGCTZs2ccUVV3DPPfdQXV3NPffcE+l3FRERyZkSCGlQxpuy8+jl8M+XW/9Ml15wx9eCnxvWwJ4HQv3Pggewa+MWqExowr0PgS9flXIV3J0LL7yQBx98kD333JN77rmHyZMnc/PNN3PVVVfx5ptv0qFDB9atW0e3bt0499xz6dy5c9IhzcWLF/PDH/4w6XUuuOACzjrrLMaPH8/NN9/MRRddxAMPPMC0adN4/PHH6d27N+vWraN9+/ZMmzaNBQsWcN1116X8PURERApKiYQ00F6frevYLQhp698OfnbsFunpP/vsMxYvXsyoUaOorq7myiuvZNWqYCWSIUOG8M1vfpNZs2axyy7Z5elnn32WM844A4AzzzyTefPmAXDUUUdx9tln87vf/S7rfUVFREQKQgmFNCjnHrVUer6ahjtHXAYLboLaidBvxLa3P8ly1qe7M2jQIJ599tmd3nv44YeZO3cuDz30EP/1X//FkiVLWj3XoEGDWLhwIUOHDm3zuk07cP3mN79h/vz5PPzww1RXV7No0aKMvoeIiEhBmHVKUU8cSEY9ai1pCmmn3gr/Njn4mXjPWgQ6dOjA+++/vy2obd68mSVLlrB161befvttRo4cyYwZM1i3bh0NDQ106dKlxXXbLr30Uv77v/+b119/HYCtW7dy9dVXA3DkkUcye/ZsAO68806OPvpoAN544w0OP/xwpk2bRo8ePXj77bdbvYaIiEjBKsGQBgpqLXvnxSCcNfWg9RsRvH7nxcguUVFRwZw5c5g4cSJDhw6lurqav/71rzQ2NjJu3DgOOeQQhg0bxve//326devGV77yFe6///6kkwmGDBnCzJkzOf300znooIMYPHjwDhMTbrnlFoYMGcIdd9zBL3/5SyAId4cccgiDBw9mxIgRDB06lJEjR/LKK69oMoGIiBSPEg1pUM5Dn205+uKdy/qN2GHoMxt1dXXbns+du3MvXdN9ZIn2339/XnrppRbPOWbMGMaMGbNTed++ffnzn/+8U/l99923U9kee+zBCy+80OI1RERECkoJhzRQj5qIiIgUqxIPaaCgJiIiIsWoDEIaaOhTREREis21NbB2WfrHFVlIgzLsUcvFpgblQm0pIiI5V0YhDcosqHXs2JG1a9cqYETA3Vm7di0dO3bMd1VERKRclFlIgzIb+uzTpw+rVq3i/fffj+R8n376aVkHlY4dO9KnT598V0NERMpBGYY0KLOg1q5dO/r16xfZ+err6xk2bFhk5xMREZEkyjSkQZkFNRERESkyM/rDxrXpH1cCIQ0U1ERERKRQTe8Fmzemf1yJhDQos8kEIiIiUiQU0gAFNRERESk0CmnbKKiJiIhI4VBI24HuURMREZHCMK0HbN2c/nElGtJAQU1EREQKwdTu4FvTP66EQxpo6FNERETyLZOQZhUlH9JAPWoiIiKST3XdgDS3drQKmPJhHLUpOOpRExERkfxQSGuTgpqIiIjknkJaSjT0KSIiIrlV1zX9Y8owpIF61ERERCSXMglp7TqVZUgDBTURERHJlUxD2uQ10delSCioiYiISPwU0jKioCYiIiLxyiSkdaoq+5AGCmoiIiISp0xD2mUroq9LEdKsTxEREYnerFNg+RPpH6eQtgP1qImIiEi0Mg1pA0YppDWjHjURERGJzrU1sHZZ+scNGAXj5kRfnyKnoCYiIiLRyDSkHT8Vjr448uqUgliHPs2sm5nNMbNXzWypmX3RzPYwsyfMbFn4s3vC5yeZ2XIze83MTkgoP8zMXg7fu8bMLM56i4iISJpm9FdIi0Hc96j9EnjM3Q8EhgJLgcuBp9x9IPBU+BozOxg4DRgEjAauN7PK8Dy/BiYAA8PH6JjrLSIiIqma3gs2rk3/uLr1CmltiC2omdnuwAjgJgB33+Tu64CxwG3hx24DTgqfjwVmu/tn7v4msBwYbma9gN3d/Vl3d+D2hGNEREQkn6b1gM0b0z+ubn30dSlBFmSfGE5sVg3cALxC0Ju2EPge8I67d0v43Ifu3t3MrgOec/dZYflNwKPASuAqdz8+LD8GmOjuY5JccwJBzxs9e/Y8bPbs2bF8tyYNDQ107tw51muUG7VptNSe0VObRkvtGb1ctumI+pMAp/n9SE2vmyeMptdzax+Ms1qRylV7jhw5cqG71zQvj3MywS7AocCF7j7fzH5JOMzZgmT3nXkr5TsXut9AEA6pqanx2tratCqcrvr6euK+RrlRm0ZL7Rk9tWm01J7Ry1mb1nWjhX+Ot0ka4OrWUxtLheKR7z+jcd6jtgpY5e7zw9dzCILbu+FwJuHP9xI+v2/C8X2A1WF5nyTlIiIikg91XWkrpCU/TsOd6YotqLn7P4G3zeyAsOg4gmHQh4DxYdl4oKn/8yHgNDPrYGb9CCYNPO/ua4ANZnZEONvzrIRjREREJJcy2RIKFNIyFPc6ahcCd5pZe2AF8B8E4fBeMzsHeAs4FcDdl5jZvQRhbgtwvrs3huc5D7gV2JXgvrVHY663iIiINJdRSDOoWxd1TcpGrEHN3RcBO90YR9C7luzz04HpScoXAIMjrZyIiIikLpOQVtEOrvgg+rqUEe31KSIiIi2bN1MhLY+0hZSIiIgkl+nm6u06weQ10denDCmoiYiIyM4y3bezUxVctiL6+pQpBTURERHZ0fReme02UDUQLlwQfX3KmO5RExERke0y3RJqwCiFtBioR01EREQC03rA1s3pH3f8VG2uHhMFNREREYGp3cG3pn+cFrKNlYKaiIhIudNuAwVL96iJiIiUM4W0gqagJiIiUq4y3hJKIS1XFNRERETKTaa7DViF9u3MMd2jJiIiUk4y3W1AW0LlhYKaiIhIuch0twGFtLxRUBMRESkHM/rDxrXpH6d9O/NKQU1ERKTUZbqQrbaEyjtNJhARESlldd0y321AIS3v1KMmIiJSqrRGWtFTj5qIiEgpUkgrCQpqIiIiJWTft+5TSCshGvoUEREpFbNOod+KDNZIA4W0AqWgJiIiUgrCNdIs3eOsAqZ8GEeNJAIKaiIiIsVuei/YvDH947SQbcFTUBMRESlmma6R1qkKLlsRfX0kUppMICIiUqwyXSOtaqBCWpFQUBMRESlGdV0B36m4zXvUBozSQrZFREOfIiIixUbLb5QN9aiJiIgUi3kz2wxpO/exhRTSipJ61ERERIpBuPxGRhTSipaCmoiISKGb0R82rk3/OK2RVvQU1ERERArZ1O7gW9M/TmuklQTdoyYiIlKo6rqmHdIcguU3FNJKgoKaiIhIIcpwZueHexym5TdKiIKaiIhIocli+Y2Xh1wRbV0kr2INama20sxeNrNFZrYgLNvDzJ4ws2Xhz+4Jn59kZsvN7DUzOyGh/LDwPMvN7BozS3vPWRERkYI36xStkSY7yEWP2kh3r3b3mvD15cBT7j4QeCp8jZkdDJwGDAJGA9ebWWV4zK+BCcDA8DE6B/UWERHJnem9YPkTmR2rkFay8jH0ORa4LXx+G3BSQvlsd//M3d8ElgPDzawXsLu7P+vuDtyecIyIiEjxm9odNm9M/zirUEgrcXEHNQf+ZGYLzWxCWNbT3dcAhD/3Cst7A28nHLsqLOsdPm9eLiIiUvzqumW2/Ea7TlojrQzEvY7aUe6+2sz2Ap4ws1db+Wyy+868lfKdTxCEwQkAPXv2pL6+Ps3qpqehoSH2a5QbtWm01J7RU5tGq9zbc0T9WGDnf+iaXjf9Y7eV7T0rDnyya29eOPx6SNJ25d6mUct3e8Ya1Nx9dfjzPTO7HxgOvGtmvdx9TTis+V748VXAvgmH9wFWh+V9kpQnu94NwA0ANTU1XltbG+G32Vl9fT1xX6PcqE2jpfaMnto0WmXbnvNmwpNT2vxYU2CrTCwbMIrdxs2htoVjyrZNY5Lv9oxt6NPMdjOzLk3PgS8Bi4GHgPHhx8YDD4bPHwJOM7MOZtaPYNLA8+Hw6AYzOyKc7XlWwjEiIiLF5dqalEJaUnXrYdycaOsjBS3OHrWewP3hShq7AHe5+2Nm9gJwr5mdA7wFnArg7kvM7F7gFWALcL67N4bnOg+4FdgVeDR8iIiIFJfpvTKbNACaNFCmYgtq7r4CGJqkfC1wXAvHTAemJylfAAyOuo4iIiI5k+mendpYvaxpZwIREZG4ZbBnJ6CZnaKgJiIiEqtMdxroVAWT10RbFyk6cS/PISIiUp5mnZL5TgMDRmnSgAAKaiIiItHTpAGJiIKaiIhIlDKdNAAKabITBTUREZGoZHo/mmZ2Sgs0mUBERCQKmYY0zeyUVqhHTUREJBvX1sDaZZkdWzUQLlwQbX2kpCioiYiIZEqTBiRmCmoiIiKZ0KQByQEFNRERkXRp0oDkiCYTiIiIpGreTE0akJxSj5qIiEgqZvSHjWszO1aTBiRDCmoiIiJtmdYDtm7O7FjdjyZZUFATERFpTaZDnaCQJlnTPWoiIiItyWbSgEKaREA9aiIiIs1ls4htu04weU209ZGypaAmIiKSKJv70QaMgnFzoq2PlDUFNRERkSZ13QDP8FgNdUr0dI+aiIgIhPejZRLSTCFNYqMeNRERKW/Z3I9W0Q6u+CDa+ogkUFATEZHylc39aFrEVnJAQU1ERMqT7keTIqCgJiIi5UeL2EqRUFATEZHyMesUWP5EZsfqfjTJg5SDmpl1B/YBPgFWuvvW2GolIiIStWzuR+tUBZetiLY+IiloNaiZWVfgfOB0oD3wPtAR6GlmzwHXu/vTsddSREQkG9kMdR4/FY6+OLKqiKSjrR61OcDtwDHuvi7xDTM7DDjTzPq7+00x1U9ERCQ7uh9NilirQc3dR7Xy3kJgYeQ1EhERiYLWR5MS0OrOBGY2LuH5Uc3euyCuSomIiGRlWo/MQ1qnKoU0KRhtbSH1g4Tn1zZ771sR10VERCR7dV0znzRw/FRNGpCC0tY9atbC82SvRURE8iebpTdA96NJQWorqHkLz5O9FhERyY/pvWDzxsyO1f1oUsDaCmoHmtlLBL1n+4XPCV/3j7VmIiIiqchmKyjt1ykFrq2gdlC2FzCzSmAB8I67jzGzPYB7gL7ASuDr7v5h+NlJwDlAI3CRuz8elh8G3ArsCjwCfM/d1aMnIlLO5s2EJ6dkfryGOqUItDqZwN3/kfgAGoBDgR7h61R8D1ia8Ppy4Cl3Hwg8Fb7GzA4GTgMGAaOB68OQB/BrYAIwMHyMTvHaIiJSimb0V0iTstDW8hx/NLPB4fNewGKC2Z53mNnFbZ3czPoAJwI3JhSPBW4Ln98GnJRQPtvdP3P3N4HlwPDwuru7+7NhL9rtCceIiEi5qesGG9dmdmy7TgppUlSstRFEM1vi7oPC5z8CDnT3s8ysC/B/7j6k1ZObzQF+CnQBLgmHPte5e7eEz3zo7t3N7DrgOXefFZbfBDxKMDx6lbsfH5YfA0x09zFJrjeBoOeNnj17HjZ79uxU2yEjDQ0NdO7cOdZrlBu1abTUntFTm0Yr3fYcUT8WaH0ZgpZmvr3Zfzxvf+7kdKtYdPRnNFq5as+RI0cudPea5uVt3aOWuBDNccDvANx9g5m1uim7mY0B3nP3hWZWm0Idky334a2U71zofgNwA0BNTY3X1qZy2czV19cT9zXKjdo0WmrP6KlNo5Vye87on3IvWtIQV7ee/YD90qteUdKf0Wjluz3bCmpvm9mFwCqCe9MeAzCzXYF2bRx7FPBVM/t3go3cdzezWcC7ZtbL3deEw5rvhZ9fBeybcHwfYHVY3idJuYiIlIOp3cFb7RtomVXAlA+jrY9IDrW1M8E5BDf3nw18I2Fj9iOAW1o70N0nuXsfd+9LMEngz+4+DngIGB9+bDzwYPj8IeA0M+tgZv0IJg087+5rgA1mdoSZGXBWwjEiIlLK6rpmHtKqBiqkSdFra1P294Bzk5Q/DTyd4TWvAu41s3OAt4BTw3MuMbN7gVeALcD57t4YHnMe25fneDR8iIhIqcpmQ3XQhAEpGa0GNTN7qLX33f2rqVzE3euB+vD5WoL73ZJ9bjowPUn5AmBwKtcSEZEiN61H5nt1YlC3LsraiORVW/eofRF4G7gbmI/29xQRkTjVdc382E5V2lBdSk5bQW1vYBRwOnAG8DBwt7svibtiIiJSRrId6jx+Khx9cWTVESkUbd2j1kgw0/MxM+tAENjqzWyau1+biwqKiEiJy2ZWp4Y6pcS11aNGGNBOJAhpfYFrgPvirZaIiJSDpgVsM9KuE0xeE11lRApQW5MJbiO4if9RYKq7L85JrUREpLSFC9i2vDdOGzTUKWWirR61M4GPgf2Bi4JlzIBgUoG7++4x1k1EREpRXTeaNphpazHP5Mdr6Q0pH23do5bRf0MiIiJJZTOrU0OdUobaGvrs7O4N2X5GRETKXBp7dSaloU4pU20NfT5oZosItmxa6O4fA5hZf2Ak8HWCjdrnxFlJEREpYq30oqW0OKeGOqWMtTq06e7HAU8B3wWWmNl6M1sLzCJYY228uyukiYjIzubNzH6oUyFNylyby3O4+yPAIzmoi4iIlIrpvWDzxjY/5rTQq6ahThEghaAmIiKSljR60bYClTuUaAFbkUQKaiIiEo0MtoHa4f4b7dUpshMFNRERyV6G20BtG/rUvWgiSaW0TpqZ3ZFKmYiIlKG6rlnu1amQJtKSVHvUBiW+MLNK4LDoqyMiIkUjxQkDLaoayNxD/h+1kVVIpPS0teDtJOBHwK5m9lFTMbAJuCHmuomISKHKZtkN2N6LVl+fdVVESllbW0j9FPipmf3U3SflqE4Fb8iUx/jos8bgxWMP56UOu3eo5KWpo/NybREpYxlMGNiBVcCUD6Orj0iJS3Xo849mtpu7f2xm44BDgV+6+z9irFtB2iGk5dFHnzXS9/Lch8RxR3yOK086JOfXFZECkOGEgW0GjIJxWiNdJB2pBrVfA0PNbChwGXATcDtwbFwVK1SFENLyadZzbzHrubfivUiSXkoFRJE8i2qoU0TSkmpQ2+LubmZjCXrSbjKz8XFWTCRRTgJiEkfttwd3fueLOb+uSMHIdsJAu04weU109REpM6kGtQ3hxIIzgWPCWZ/t4quWSGH4vzf+lZch5o6VxqvT/z3n1xXZgXrRRPIu1aD2DeAM4Fvu/k8z+xzw8/iqVbh271BZ9sOfEr9PGz26gJjmhJeZ36jmpGG9o7m2FKcZ/WHj2syP14QBkcikFNTCcHYn8AUzGwM87+63x1u1wvTS1NEFM6FAJA4X37OIi+9ZlJdrr7zqxLxcVxJk24umCQMikUopqJnZ1wl60OoJ1lG71swudfey/K+xaVmM+vp6amtrc3JNhUMpB/kYZm5S9iEx22U3QEOdIjFIdehzMvAFd38PwMz2BJ4EyjKo5UO+1kzrd/nDeF6uLJJbaYXEiNdPzPus5rpukM1/6ZowIBKbVINaRVNIC60lxX1Cpbi9meNehqZeynz2rIjkWr5mNX+38g9MrLybimz+NlcvmkisUg1qj5nZ48Dd4etvAI/GUyWR/A1DqQdRysXr7c9kFxoxA2/2h95s+/Pm7zWVbaGS/TfdAVn+T9U+u8Ffa7M6hUhJS3UywaVmdjJwNME9aje4+/2x1kwkD3Ldg9jk8OlP8O6GTXm5tpSXpl402DGQNWkqSxbQtnrwD8DPGk/nt41fiaQ+qz/O772JWitRCl1bm7IPAHq6+/+5+33AfWH5CDPbz93fyEUlRUrd/MmjYjlvKhNeNMxcPhJ70ZpLpRcNoP+mu+KpXJ7ka63EJtq3WdrSVo/aTOBHSco3hu9F879UIpI3+RpmfuBv7+RtGZBytKL9GUDmvWgr6MXxm34RXwXLVGz7Nqcx4aXsZzwXuLaCWl93f6l5obsvMLO+8VRJRMrBScN6521h3XIKiUvan82ubMKdnSYNpNKLZpReL5rsKN+96tqJpXVtBbWOrby3a5QVERHJlWxCYhTrJ+bqH8bEXjQz2Lp1e1hLpRftX3ShZtNvc1JXKV+R7sSSoUIegm4rqL1gZt9x998lFprZOcDC1g40s47AXKBDeJ057j7FzPYA7gH6AiuBr7v7h+Exk4BzgEbgInd/PCw/DLiVIBw+AnzPPdlfLyIihS/2oaYWNlKvrNzxtXrRRAJtDkGHQ8n5GCZuK6hdDNxvZt9kezCrAdoDX2vj2M+Af3P3BjNrB8wzs0eBk4Gn3P0qM7scuByYaGYHA6cBg4B9gCfNbH93bwR+DUwAniMIaqPR8iAiIjtLYwuopPerVbaDKz5gZXQ12sGASQ+zRf+bLUWq7+UP5zystRrU3P1d4EgzGwkMDosfdvc/t3XisMerIXzZLnw4MBaoDctvI9iWamJYPtvdPwPeNLPlwHAzWwns7u7PApjZ7cBJKKiJiGzXQi9aWnKweO3yn+74j1wut+IDrZUoxcfiHEE0s0qCnrgBwK/cfaKZrXP3bgmf+dDdu5vZdcBz7j4rLL+JIIytBK5y9+PD8mOAie4+Jsn1JhD0vNGzZ8/DZs+eHdt3A2hoaKBz586xXqPcqE2jpfaMXiG26Yj6sUAwZNlcU1myv+m3hu+7VfLMsffFU7k2FGJ7xum8Jz7mk9i3bXaS/2mQKNw6erdYzjty5MiF7l7TvDzVnQkyEg5bVptZN4Ih1MGtfDzZn6qW/rQlTZfufgNwA0BNTY3H/X9puf4/wXKgNo2W2jN6BdWm03rA1s0pfTTZX6SVsK0XrTaqOqWpoNozB5bWxn+N1tq0nGY8xyXXf15jDWpN3H2dmdUT3Fv2rpn1cvc1ZtYLaNpDdBWwb8JhfYDVYXmfJOUiIuUrjXvRkrIKmPJhNHWRopHPZXGaaCeW9MQW1MxsT2BzGNJ2BY4HfgY8BIwHrgp/Phge8hBwl5ldTTCZYCDwvLs3mtkGMzsCmA+cBVwbV71FRApaGr1oLdJG6pJHce3Eko4hUx7jo8/SH4MuxFmf2egF3Bbep1YB3OvufzSzZ4F7wyU+3gJOBXD3JWZ2L/AKsAU4Pxw6BTiP7ctzPIomEohIuZk3E56ckt051IsmApDWmmn5Hp6PLaiFOxoMS1K+FjiuhWOmA9OTlC9g+6xTEZHyMrU7+NbsznH8VDj64kiqIyK5k5N71EREJANR9KJVBOuiiUhxUlATESlEdd1oYYJ7GufQvWgixU5BTUSkkFxbA2uXZXcO9aKJlAwFNRGRQpHtkhugXjSREqOgJiKSbzP6w8a12Z2jXSeYvCaa+ohIwVBQExHJJ/WiiUgrFNRERPIhiiU31IsmUvIU1EREcmnWKbD8iezPo140kbKgoCYikitRDHNWDYQLF2R/HhEpCgpqIiJxm94LNm/M/jzqRRMpOxX5roCISEmr65p9SBswSiFNpEypR01EJA5RTBbAoG5dFLURkSKloCYiEqUodhYA9aCJCKCgJiISnSgmC2jJDRFJoKAmIpKtaT1g6+bsz6NeNBFpRkFNRCRTUa2JNmAUjJuT/XlEpOQoqImIZCKKYU6rgCkfZn8eESlZCmoiImk45i8nQ31j9ifSMKeIpEBBTUQkFeEwp2V7Hk0WEJE0KKiJiLQlYZgzq6CmXjQRSZOCmohISyJZtBY4fiocfXH25xGRsqOgJiLSXFSL1la0gys+yP48IlK2FNRERBK1MZsz5aFPDXOKSAQU1EREIJrlNkBroolIpBTURKS8Te8Fmzem/HGnhV41DXOKSAwU1ESkPEW1qwBomFNEYqOgJiLlJ6phTs3mFJGYKaiJSPmo60YweJk5B0yL1opIjiioiUjpS/M+tNbMrX2Q2traSM4lItIWBTURKV1RrYcG2+9Dq6+P5nwiIilQUBOR0hTVfWhVA+HCBdGcS0QkTQpqIlJaogpoVgFTPozmXCIiGVJQE5HSENW+nKDlNkSkYFTEdWIz29fMnjazpWa2xMy+F5bvYWZPmNmy8Gf3hGMmmdlyM3vNzE5IKD/MzF4O37vGzFLexUVEStz0XkEvWlSbpyukiUgBibNHbQvwQ3d/0cy6AAvN7AngbOApd7/KzC4HLgcmmtnBwGnAIGAf4Ekz29/dG4FfAxOA54BHgNHAozHWXUQKXZQTBTpVwWUrojmXiEiEYgtq7r4GWBM+32BmS4HewFigNvzYbUA9MDEsn+3unwFvmtlyYLiZrQR2d/dnAczsduAkFNREylOUOwroPjQRKXA5uUfNzPoCw4D5QM8wxOHua8xsr/BjvQl6zJqsCss2h8+bl4tIuYlqogBoiFNEikLsQc3MOgO/By52949aub0s2Rst7X+cdGlxM5tAMERKz549qY95vaOGhobYr1Fu1KbRKpX2HFE/Fgj+w092Y23iXxIt7TvQVD639sHgSYbtUiptWijUntFTm0Yr3+0Za1Azs3YEIe1Od78vLH7XzHqFvWm9gPfC8lXAvgmH9wFWh+V9kpTvxN1vAG4AqKmp8bhXD6+vr9cK5RFTm0ar6Nszgy2fWvpfQRswCsbN2XbfRaaKvk0LjNozemrTaOW7PWMLauHMzJuApe5+dcJbDwHjgavCnw8mlN9lZlcTTCYYCDzv7o1mtsHMjiAYOj0LuDaueotIAYhyqQ1NFBCRIhZnj9pRwJnAy2a2KCz7EUFAu9fMzgHeAk4FcPclZnYv8ArBjNHzwxmfAOcBtwK7Ekwi0EQCkVI0rQds3RzNuTRRQERKQJyzPufR8ijEcS0cMx2YnqR8ATA4utqJSEGJcNN0QBMFRKRkaGcCEcmfGf1h49rozqeAJiIlRkFNRHIvysVqQQFNREqWgpqI5E6Ui9UChDM5RURKlYKaiMQv6h60qoFw4YLoziciUqAU1EQkPlH3oLXrBJPXRHc+EZECp6AmItGLOqBVtIMrPojufCIiRUJBTUSio4AmIhIpBTURyV7UAQ2DunURnk9EpDgpqIlI5qKeJABaakNEJIGCmoikL+qFakEBTUQkCQU1EUmdApqISE4pqIlI26LcLL2JApqISJsU1ESkZQpoIiJ5paAmIjur6wZ4xOdUQBMRSZeCmohsV9c1hnMqoImIZEpBTUQU0ERECpSCmki5inyRWtBCtSIi0VJQEyk3cSyxoYAmIhILBTWRchHHDE7txSkiEisFNZFSF8f9ZwpoIiI5oaAmUqrqujIi6nN2qoLLVkR9VhERaYGCmkgpabZJukV13gGjYNycqM4mIiIpUlATKQVTu4Nv3anYyTKsHT8Vjr44mzOIiEgWFNREilkb959lHNK0BpqISEFQUBMpNs2GN1uTXlDTEhsiIoVGQU2kWLQwvNmalIY+NYNTRKRgKaiJFLo4ltcAqBoIFy6I59wiIhIJBTWRQjS9F2zeGM+5df+ZiEjRUFATKSQR955tH/rU/WciIsVIQU0k39KYHJCurRUdqLjivVjOLSIi8VNQE8mXuO49g20L1M6rr6c2vquIiEjMFNREcmlGf9i4Nr7z6/4zEZGSoqAmkgtx9p616wST18R3fhERyRsFNZG4qPdMRESyVBHXic3sZjN7z8wWJ5TtYWZPmNmy8Gf3hPcmmdlyM3vNzE5IKD/MzF4O37vGzCLbZ1okFnVdg0ccIc0qgoCmkCYiUhZiC2rArcDoZmWXA0+5+0DgqfA1ZnYwcBowKDzmejOrDI/5NTABGBg+mp9TJP/qum0PaHGoGhiEsykfxnN+EREpSLENfbr7XDPr26x4LGybhHYbUA9MDMtnu/tnwJtmthwYbmYrgd3d/VkAM7sdOAl4NK56i6Qs7qFNUM+ZiEiZy/U9aj3dfQ2Au68xs73C8t7AcwmfWxWWbQ6fNy9PyswmEPS+0bNnT+rr66OreRINDQ2xX6PcFEObjqgfu+15a+Pwzd/zVj6b+N4nu/bmhcOvD15k2RbF0J7FRm0aLbVn9NSm0cp3exbKZIJk/961tJ90i//eufsNwA0ANTU1XltbG0nlWlJfX0/c1yg3BdumEQxpth7qtu8csBtEtvZZwbZnEVObRkvtGT21abTy3Z65DmrvmlmvsDetF9C0ZPoqYN+Ez/UBVoflfZKUi8RvanfwrfFeI1yYVkREJJlcB7WHgPHAVeHPBxPK7zKzq4F9CCYNPO/ujWa2wcyOAOYDZwHX5rjOUk6m9YCtm+O9hlVoUoCIiKQktqBmZncTjOD0MLNVwBSCgHavmZ0DvAWcCuDuS8zsXuAVYAtwvrs3hqc6j2AG6a4Ekwg0kUCilYtJAaCJASIikrY4Z32e3sJbx7Xw+enA9CTlC4DBEVZNJNaN0HdQNRAuXBD/dUREpCQVymQCkfjNOgWWP5GDC22fGCAiIpINBTUpbbnqOQMNbYqISOQU1KT05OqeM9CsTRERiZWCmpSGXMzWbNKuE0xek5triYhIWVNQk+KVi3XOmlS0gys+yM21REREQgpqUlzi2vQ8KU0KEBGR/FJQk8KWy8kATTQpQERECoSCmhSeXA5pNlE4ExGRAqSgJvk3byY8OQWAEbm8rsKZiIgUOAU1yY8Wes0szmtqj00RESkyCmqSG/m41ww0W1NERIqagprEJ4MZmpH0qHWqgstWRHEmERGRvFJQk+jkdOmMZo6fCkdfnL/ri4iIxEBBTTIXQzBz0uhV02QAEREpcQpqkrp89piBtm4SEZGyo6AmLctDMNupR029ZiIiUsYU1CSQr1mZzWzZZXfa//jtfFdDRESkICiolat8D2MmSug1+2t9PbX5q4mIiEhBUVArB9N6wNbN+a7FdhrOFBERSYmCWqmZ3gs2b8x3LXZUNRAuXJDvWoiIiBQdBbViVmg9ZU0UzERERCKhoFYsCumesuYGjIJxc/JdCxERkZKjoFZoWtisvKDoHjMREZGcUFDLl0LuIWtOwUxERCQvFNTiVIg39rdFw5giIiIFQ0EtW3XdCNbTL0IV7eCKD/JdCxEREWmBglq6EmZajshzVdJiFTDlw3zXQkRERNKgoJauPfaDD14Fmu1JWUjUUyYiIlISFNTSdcF8uO7wbWEt746fCkdfnO9aiIiISAwU1DJxwXyo65rbHrVOVXDZilxeUURERPJMQS0T1x0OBFMIIg9rmnUpIiIiIQW1dEUx7Kkb+0VERCQFCmrp+tcb255u61Frtxts/njHz2moUkRERLJUNEHNzEYDvwQqgRvd/aq8VCRhNuXc+npqa2vzUg0REREpfRX5rkAqzKwS+BXwZeBg4HQzOzi/tRIRERGJV1EENWA4sNzdV7j7JmA2MDbPdRIRERGJVbEEtd7A2wmvV4VlIiIiIiXL3At/n0ozOxU4wd2/Hb4+Exju7hc2+9wEYAJAz549D5s9e3as9WpoaKBz586xXqPcqE2jpfaMnto0WmrP6KlNo5Wr9hw5cuRCd69pXl4skwlWAfsmvO4DrG7+IXe/AbgBoKamxuO+0b9ekwkipzaNltozemrTaKk9o6c2jVa+27NYhj5fAAaaWT8zaw+cBjyU5zqJiIiIxKooetTcfYuZXQA8TrA8x83uviTP1RIRERGJVVEENQB3fwR4JN/1EBEREcmVYhn6FBERESk7CmoiIiIiBaoolufIhJm9D/wj5sv0AD5o81OSDrVptNSe0VObRkvtGT21abRy1Z6fd/c9mxeWbFDLBTNbkGzNE8mc2jRaas/oqU2jpfaMnto0WvluTw19ioiIiBQoBTURERGRAqWglp0b8l2BEqQ2jZbaM3pq02ipPaOnNo1WXttT96iJiIiIFCj1qImIiIgUKAW1DJjZaDN7zcyWm9nl+a5PITOzm83sPTNbnFC2h5k9YWbLwp/dE96bFLbra2Z2QkL5YWb2cvjeNWZmuf4uhcDM9jWzp81sqZktMbPvheVq0wyZWUcze97M/h626dSwXG2aBTOrNLO/mdkfw9dqzyyY2cqwLRaZ2YKwTG2aITPrZmZzzOzV8O/TLxZse7q7Hmk8CPYafQPoD7QH/g4cnO96FeoDGAEcCixOKJsBXB4+vxz4Wfj84LA9OwD9wnauDN97HvgiYMCjwJfz/d3y1J69gEPD512A18N2U5tm3qYGdA6ftwPmA0eoTbNu1x8AdwF/DF+rPbNrz5VAj2ZlatPM2/M24Nvh8/ZAt0JtT/WopW84sNzdV7j7JmA2MDbPdSpY7j4X+Fez4rEE/5EQ/jwpoXy2u3/m7m8Cy4HhZtYL2N3dn/Xgv4zbE44pK+6+xt1fDJ9vAJYCvVGbZswDDeHLduHDUZtmzMz6ACcCNyYUqz2jpzbNgJntTtCJcBOAu29y93UUaHsqqKWvN/B2wutVYZmkrqe7r4EgeAB7heUttW3v8Hnz8rJmZn2BYQQ9QGrTLITDdIuA94An3F1tmp2ZwGXA1oQytWd2HPiTmS00swlhmdo0M/2B94FbwuH5G81sNwq0PRXU0pds/FlTZ6PRUtuqzZsxs87A74GL3f2j1j6apExt2oy7N7p7NdCH4P+UB7fycbVpK8xsDPCeuy9M9ZAkZWrPnR3l7ocCXwbON7MRrXxWbdq6XQhuyfm1uw8DPiYY6mxJXttTQS19q4B9E173AVbnqS7F6t2wy5jw53theUttuyp83ry8LJlZO4KQdqe73xcWq00jEA5/1AOjUZtm6ijgq2a2kuDWkH8zs1moPbPi7qvDn+8B9xPchqM2zcwqYFXYcw4whyC4FWR7Kqil7wVgoJn1M7P2wGnAQ3muU7F5CBgfPh8PPJhQfpqZdTCzfsBA4PmwC3qDmR0Rzqg5K+GYshJ+/5uApe5+dcJbatMMmdmeZtYtfL4rcDzwKmrTjLj7JHfv4+59Cf5+/LO7j0PtmTEz283MujQ9B74ELEZtmhF3/yfwtpkdEBYdB7xCobZnLmZXlNoD+HeC2XZvAJPzXZ9CfgB3A2uAzQT/93EOUAU8BSwLf+6R8PnJYbu+RsLsGaCG4C+mN4DrCBdrLrcHcDRB1/pLwKLw8e9q06zadAjwt7BNFwNXhOVq0+zbtpbtsz7Vnpm3Y3+CWYd/B5Y0/bujNs2qTauBBeF/9w8A3Qu1PbUzgYiIiEiB0tCniIiISIFSUBMREREpUApqIiIiIgVKQU1ERESkQCmoiYiIiBQoBTURyRszczP7RcLrS8ysLqJz32pmp0Rxrjauc6qZLTWzp5uV9zWzT8xsUcLjrAivW2tmf4zqfCJSmHbJdwVEpKx9BpxsZj919w/yXZkmZlbp7o0pfvwc4D/d/ekk773hwdZUIiIZUY+aiOTTFuAG4PvN32jeI2ZmDeHPWjP7i5nda2avm9lVZvZNM3vezF42s/0STnO8mT0Tfm5MeHylmf3czF4ws5fM7LsJ533azO4CXk5Sn9PD8y82s5+FZVcQLEL8GzP7eapf2swazOwXZvaimT1lZnuG5dVm9lxYr/vNrHtYPsDMnjSzv4fHNH3HzmY2x8xeNbM7w9XRCdvklfA8/y/VeolI4VFQE5F8+xXwTTPrmsYxQ4HvAYcAZwL7u/tw4EbgwoTP9QWOBU4kCFMdCXrA1rv7F4AvAN8Jt4WBYP/Eye5+cOLFzGwf4GfAvxGsaP4FMzvJ3acRrG7+TXe/NEk992s29HlMWL4b8KIHm2z/BZgSlt8OTHT3IQRhsan8TuBX7j4UOJJgtw+AYcDFwMEEq9cfZWZ7AF8DBoXnubL1phSRQqagJiJ55e4fEQSUi9I47AV3X+PunxFs3fKnsPxlgnDW5F533+ruy4AVwIEE+ySeZWaLgPkE28YMDD//vLu/meR6XwDq3f19d99CEJxGpFDPN9y9OuHxTFi+FbgnfD4LODoMqt3c/S9h+W3AiHCPx97ufj+Au3/q7hsT6rvK3bcSbCfWF/gI+BS40cxOBpo+KyJFSEFNRArBTIKert0SyrYQ/h0VDum1T3jvs4TnWxNeb2XHe2+b75HngAEXJoSnfu7eFPQ+bqF+luL3yFRre/m1du3EdmgEdgmD5HDg98BJwGNZ105E8kZBTUTyzt3/BdxLENaarAQOC5+PBdplcOpTzawivKerP8GGyo8D55lZOwAz29/MdmvtJAQ9b8eaWQ8zqwROJxiyzFQF0HT/3RnAPHdfD3yYMDx6JvCXsMdxlZmdFNa3g5l1aunEZtYZ6OrujxAMi1ZnUU8RyTPN+hSRQvEL4IKE178DHjSz54GnaLm3qzWvEQSqnsC57v6pmd1IMET4YthT9z5Bz1OL3H2NmU0Cnibo4XrE3R9M4fr7hUOsTW5292sIvssgM1sIrAe+Eb4/nuBeuk4EQ7X/EZafCfzWzKYBm4FTW7lmF4J26xjWdaeJGiJSPMy9tR53ERGJmpk1uHvnfNdDRAqfhj5FRERECpR61EREREQKlHrURERERAqUgpqIiIhIgVJQExERESlQCmoiIiIiBUpBTURERKRAKaiJiIiIFKj/D/2ZJAZ70ijgAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Plot_data(train_costs, test_costs):\n",
    "    \"\"\"\n",
    "    Plot the cost (MSE) for both the train and test sets after each epoch.\n",
    "\n",
    "    Args:\n",
    "    train_costs (list): List of training cost values for each epoch.\n",
    "    test_costs (list): List of test cost values for each epoch.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(train_costs) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, train_costs, label='Train Cost', marker='o', linestyle='-')\n",
    "    plt.plot(epochs, test_costs, label='Test Cost', marker='x', linestyle='-')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Cost (MSE)')\n",
    "    plt.title('Cost vs. Number of Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Usage:\n",
    "# Assuming you have lists of training and test costs obtained during training\n",
    "Plot_data(train_cost_history, test_cost_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load your dataset (replace 'your_dataset.csv' with your dataset)\n",
    "# Ensure that your dataset has at least 300 training examples and more than 5 features\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Step 2: Preprocess the data if necessary (e.g., handle missing values, feature scaling)\n",
    "\n",
    "# Step 3: Split the dataset into training and test sets\n",
    "X = data.drop('dependent_variable', axis=1)  # Assuming 'dependent_variable' is the target variable\n",
    "Y = data['dependent_variable']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Implement and train the linear regression model with different hyperparameters\n",
    "\n",
    "def LinearRegression(X_train, Y_train, X_test, Y_test, learning_rate, num_epochs):\n",
    "    # Initialize model parameters (weights and bias)\n",
    "    weights = np.random.rand(X_train.shape[1])\n",
    "    bias = 0.0\n",
    "\n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Perform gradient descent and update model parameters\n",
    "        updated_weights, updated_bias, cost_history = Gradient_descent(X_train, Y_train, weights, bias, learning_rate, num_epochs)\n",
    "\n",
    "        # Calculate cost for training set\n",
    "        train_cost = Cost_function(X_train, Y_train, updated_weights, updated_bias)\n",
    "        train_costs.append(train_cost)\n",
    "\n",
    "        # Calculate cost for test set\n",
    "        test_cost = Cost_function(X_test, Y_test, updated_weights, updated_bias)\n",
    "        test_costs.append(test_cost)\n",
    "\n",
    "    return train_costs, test_costs\n",
    "\n",
    "# Step 5: Monitor and record the costs for different hyperparameter combinations\n",
    "learning_rates = [0.01, 0.1, 0.5]  # You can experiment with different learning rates\n",
    "num_epochs_values = [100, 500, 1000]  # You can experiment with different numbers of epochs\n",
    "\n",
    "best_learning_rate = None\n",
    "best_num_epochs = None\n",
    "best_test_cost = float('inf')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in num_epochs_values:\n",
    "        train_costs, test_costs = LinearRegression(X_train, Y_train, X_test, Y_test, lr, epochs)\n",
    "\n",
    "        # Step 6: Plot the costs for different hyperparameter combinations\n",
    "        plt.plot(range(1, epochs + 1), train_costs, label=f'Train Cost (lr={lr}, epochs={epochs})')\n",
    "        plt.plot(range(1, epochs + 1), test_costs, label=f'Test Cost (lr={lr}, epochs={epochs})')\n",
    "\n",
    "        # Step 7: Find the best hyperparameters based on the lowest test cost\n",
    "        if test_costs[-1] < best_test_cost:\n",
    "            best_test_cost = test_costs[-1]\n",
    "            best_learning_rate = lr\n",
    "            best_num_epochs = epochs\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Cost vs. Number of Epochs for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 7 (continued): Report the best values of learning_rate and num_epochs\n",
    "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
    "print(f\"Best Number of Epochs: {best_num_epochs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load your dataset (replace 'your_dataset.csv' with your dataset)\n",
    "# Ensure that your dataset has at least 300 training examples and more than 5 features\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Step 2: Preprocess the data if necessary (e.g., handle missing values, feature scaling)\n",
    "\n",
    "# Step 3: Split the dataset into training and test sets\n",
    "X = data.drop('dependent_variable', axis=1)  # Assuming 'dependent_variable' is the target variable\n",
    "Y = data['dependent_variable']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Step 4: Implement the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Step 5: Update the cost function to use cross-entropy for logistic regression\n",
    "def Cost_function(X, Y, weights, bias):\n",
    "    predicted_probabilities = sigmoid(np.dot(X, weights) + bias)\n",
    "    cost = -np.mean(Y * np.log(predicted_probabilities) + (1 - Y) * np.log(1 - predicted_probabilities))\n",
    "    return cost\n",
    "\n",
    "# Step 6: Update the gradient descent function for logistic regression\n",
    "def Gradient_descent(X, Y, weights, bias, learning_rate, num_epochs):\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        predicted_probabilities = sigmoid(np.dot(X, weights) + bias)\n",
    "\n",
    "        # Compute gradients for weights and bias\n",
    "        gradient_weights = (1 / len(Y)) * np.dot(X.T, (predicted_probabilities - Y))\n",
    "        gradient_bias = (1 / len(Y)) * np.sum(predicted_probabilities - Y)\n",
    "\n",
    "        # Update weights and bias using gradients and learning rate\n",
    "        weights -= learning_rate * gradient_weights\n",
    "        bias -= learning_rate * gradient_bias\n",
    "\n",
    "        # Calculate the cost and append it to the cost history\n",
    "        cost = Cost_function(X, Y, weights, bias)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print the cost for every 100 epochs (optional)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Cost = {cost}\")\n",
    "\n",
    "    return weights, bias, cost_history\n",
    "\n",
    "# Step 7: Monitor and record the costs for different hyperparameter combinations\n",
    "learning_rates = [0.01, 0.1, 0.5]  # You can experiment with different learning rates\n",
    "num_epochs_values = [100, 500, 1000]  # You can experiment with different numbers of epochs\n",
    "\n",
    "best_learning_rate = None\n",
    "best_num_epochs = None\n",
    "best_test_cost = float('inf')\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for epochs in num_epochs_values:\n",
    "        # Initialize model parameters (weights and bias)\n",
    "        weights = np.random.rand(X_train.shape[1])\n",
    "        bias = 0.0\n",
    "\n",
    "        # Perform gradient descent and update model parameters\n",
    "        updated_weights, updated_bias, cost_history = Gradient_descent(X_train, Y_train, weights, bias, lr, epochs)\n",
    "\n",
    "        # Calculate cost for test set\n",
    "        test_cost = Cost_function(X_test, Y_test, updated_weights, updated_bias)\n",
    "\n",
    "        # Step 8: Find the best hyperparameters based on the lowest test cost\n",
    "        if test_cost < best_test_cost:\n",
    "            best_test_cost = test_cost\n",
    "            best_learning_rate = lr\n",
    "            best_num_epochs = epochs\n",
    "\n",
    "        # Plot the cost for both train and test sets\n",
    "        plt.plot(range(1, epochs + 1), cost_history, label=f'Cost (lr={lr}, epochs={epochs})')\n",
    "\n",
    "# Plot settings\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Cost (Cross-Entropy)')\n",
    "plt.title('Cost vs. Number of Epochs for Different Hyperparameters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 8 (continued): Report the best values of learning_rate and num_epochs\n",
    "print(f\"Best Learning Rate: {best_learning_rate}\")\n",
    "print(f\"Best Number of Epochs: {best_num_epochs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
